# -*- coding: utf-8 -*-
"""Assignment_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1huLCt59NunxYbuwPxkw_p12Y3vFF1zi_
"""

import os
import cv2
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler,StandardScaler
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score,confusion_matrix

"""PART 1"""

def PCA_Gray(Data,dim = 3):
    means = []

    for img in Data:
        means.append(img.mean())

    image_Arr = np.array(Data)
    means_Arr = np.array(means).reshape(1,len(Data))

    normalized_img = np.subtract(image_Arr.T , means_Arr)

    covariance_m = np.matmul(normalized_img.T,normalized_img)

    val,vec = np.linalg.eig(covariance_m)

    index = val.argsort()

    selected_vecs = []

    for j in range(len(index)-1,len(index)-dim-1,-1):
        selected_vecs.append(vec[index[j]])

    selected_vecs = np.array(selected_vecs)

    projection = np.matmul(normalized_img,selected_vecs.T)

    features = []
    for img in image_Arr:
        img = img.reshape(1,img.shape[0])
        img = np.matmul(img,projection).flatten()
        features.append(img)



    return np.array(features)

img_arr = []

# Reading images from directory
for img in os.listdir("Dataset1"):
    image = cv2.imread("Dataset1/"+img,0)
    image = cv2.resize(image,(256,256)).flatten()
    image = image / 255
    img_arr.append(image)



features = PCA_Gray(img_arr)
print(features)
# Scaling PCA features between 0 and 1
Scaler = MinMaxScaler()
features = Scaler.fit_transform(features)

img_names = [i for i in os.listdir("Dataset1")]

col_names = [("PC " + str(i+1)) for i in range(len(features[0]))]


df = pd.DataFrame(features,index=img_names,columns=col_names)

df

"""PART 2

PREPARING DATA
"""

train_img = []
train_labels = []
train_dict = {}


for foldername in os.listdir("Dataset2")[:-1]:

    train_labels.append(foldername)
    for filename in os.listdir("Dataset2/"+foldername):

        Path = "Dataset2/" + foldername +"/"+filename
        img = cv2.imread(Path,1)
        img = cv2.resize(img,(200,200))


        train_img.append(img)




for i in range(len(train_labels)):
    train_dict[i] = train_labels[i]

print(train_dict)

query_img = []
query_labels = []
query_dict = {}
Path = "Dataset2/QUERY_IMAGES"
for filename in os.listdir(Path):
    img = cv2.imread(Path+"/"+filename,1)
    img = cv2.resize(img,(200,200))

    query_img.append(img)
    query_labels.append(filename.split("_")[0])


for i in range(len(query_labels)):
    query_dict[i] = query_labels[i]

"""Implementing Necessary Functions"""

def PCA_RGB(Data,dim = 3):

    # Storing each channel's values
    Red = []
    Green = []
    Blue = []

    for img in Data:

        for channel in range(3):
            channel_data = img[:,:,channel].flatten()
            if(channel == 0):
                Blue.append(channel_data)
            elif(channel ==1):
                Green.append(channel_data)
            else:
                Red.append(channel_data)

    # Converting each channel to their PCA components
    features_Red = PCA_Gray(Red)
    features_Green = PCA_Gray(Green)
    features_Blue = PCA_Gray(Blue)

    Scaler = MinMaxScaler()

    features_Red = Scaler.fit_transform(features_Red)
    features_Green = Scaler.fit_transform(features_Green)
    features_Blue = Scaler.fit_transform(features_Blue)

    # Concatenating pca features
    features = np.array(list(zip(features_Red,features_Green,features_Blue))) # (300,3,3)

    features = features.reshape(features.shape[0],9)  # (300,9)

    return features

def euclidean(x1,x2):
    sum_of_squares = 0

    for i in range(len(x1)):

        diff = x1[i] - x2[i]

        sum_of_squares += diff**2

    return sum_of_squares ** 0.5

def histogram_RGB(image):
    hist = []
    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)
    for channel in range(2,-1,-1):
        histogram = np.zeros(256)
        for pixel in range(256):
            histogram[pixel] = np.count_nonzero(image[:,:,channel] == pixel)

        hist.append(histogram)

    return np.array(hist)

def histogram_HSV(image):
    hist = []
    image = cv2.cvtColor(image,cv2.COLOR_BGR2HSV)

    for channel in range(2,-1,-1):
        histogram = np.zeros(256)
        for pixel in range(256):
            histogram[pixel] = np.count_nonzero(image[:,:,channel] == pixel)

        hist.append(histogram)

    return np.array(hist)

def Show_Ten_Nearest(train,test,query_dict,train_dict,query_len = 20, train_len = 300):
    # For each query image, distance between each train image is calculated and stored in distances list. Then distances list is sorted and 10 nearest image is shown.
    for i in range(query_len):
        distances = []
        for j in range(train_len):

            label = train_dict[ j // 30 ]
            distance = euclidean(test[i],train[j])

            distances.append((distance,label))

        distances.sort()

        ranked_list = distances[:10]
        print(f"10 Nearest label for: {query_dict[i]}\n")
        for k in ranked_list:
            print(f"Distance: {k[0]} , Class: {k[1]}")
        print("\n*******************************************\n")

def calculateMAP(train,test,query_labels,train_dict,query_len = 20,train_len = 300):
    precisions = [[] for i in range(query_len)] # Empty lists for each query class (2 for each)


    for i in range(query_len):  # Looping through query images
        distances = [] # Distance list for each query image
        for j in range(train_len): # Inner loop for train images
            label = train_dict[ j // 30 ]  # Label of train image
            distance = euclidean(test[i],train[j])

            distances.append((distance,label)) # Distances are stored with their labels.

        distances.sort()


        # Precision calculations for each query image


        k = 0   # Retrieved images
        found = 0 # Relevant images

        while(found<30): # Since there are 30 relevant image for each class, loop ends at when found = 30
            nearest_img_label = distances[k][1] # Image at the K. rank

            if(nearest_img_label == query_labels[i]): # If image is relevant

                found+=1
                precision = found / (k+1)

                precisions[i].append(precision)
            k+=1


    # Taking mean of each precision list
    for i in range(len(precisions)):
        precisions[i] = np.array(precisions[i]).mean()



    # MAP calculation for each class
    MAP_scores = []
    for i in range(0,len(precisions),2):
        MAP = (precisions[i] + precisions[i+1]) / 2
        MAP_scores.append(MAP)



    for i in range(len(MAP_scores)):
        print(f"MAP Score: {MAP_scores[i]},\tLabel: {train_dict[i]}")

def K_means(train,num_clusters = 10):
    scaler = StandardScaler()
    k_features = scaler.fit_transform(train)


    kmeans = KMeans(num_clusters)
    x=kmeans.fit_predict(k_features)


    score = silhouette_score(train,kmeans.labels_)
    print(f"Silhoutte Score of Model: {score}")

"""2.1) PCA Features"""

features_train = PCA_RGB(train_img) # Shape:(300,9)
features_query = PCA_RGB(query_img) # Shape:(20,9)

# Showing 10 nearest image for each query image.
Show_Ten_Nearest(features_train,features_query,query_dict,train_dict)

# MAP Calculation using PCA features
calculateMAP(features_train,features_query,query_labels,train_dict)

# K-Means for PCA features
K_means(features_train)

"""2.2) Color Histogram Features"""

# Testing for single image (RGB)

test_img = train_img[0]

image = cv2.cvtColor(test_img,cv2.COLOR_BGR2RGB)

hist = histogram_RGB(image)

plt.plot(hist[0],color="red",label="Red channel")
plt.plot(hist[1],color = "green",label = "Green channel")
plt.plot(hist[2],color="blue",label="Blue channel")
plt.legend()

# Testing for single image (HSV)

test_img = train_img[0]


hist = histogram_HSV(test_img)

plt.plot(hist[0],color="red",label="Hue")
plt.plot(hist[1],color = "green",label="Satiration")
plt.plot(hist[2],color="blue",label="Value")
plt.legend()

train_hist_RGB = []
query_hist_RGB = []

train_hist_HSV = []
query_hist_HSV = []

for img in train_img:
    train_hist_RGB.append(histogram_RGB(img))
    train_hist_HSV.append(histogram_HSV(img))

for img in query_img:
    query_hist_RGB.append(histogram_RGB(img))
    query_hist_HSV.append(histogram_HSV(img))


# Flattening histogram information
train_hist_RGB = np.array(train_hist_RGB).reshape((300,256*3))
train_hist_HSV = np.array(train_hist_HSV).reshape((300,256*3))


query_hist_RGB = np.array(query_hist_RGB).reshape((20,256*3))
query_hist_HSV = np.array(query_hist_HSV).reshape((20,256*3))

# Showing 10 nearest image for each query image. (RGB)

Show_Ten_Nearest(train_hist_RGB,query_hist_RGB,query_dict,train_dict)

# Showing 10 nearest image for each query image. (HSV)

Show_Ten_Nearest(train_hist_HSV,query_hist_HSV,query_dict,train_dict)

# MAP score for histogram features (RGB)
calculateMAP(train_hist_RGB,query_hist_RGB,query_labels,train_dict)

# MAP score for histogram features (HSV)
calculateMAP(train_hist_HSV,query_hist_HSV,query_labels,train_dict)

# K-Means for histogram features (RGB)
K_means(train_hist_RGB)

# K-Means for histogram features (HSV)
K_means(train_hist_HSV)

"""PART 3) LOGISTIC REGRESSION"""

class LogisticRegression: # Logistic Regression for two classes.
    def __init__(self,learning_Rate,iterations):
        self.lr = learning_Rate
        self.weights = 0
        self.bias = 0
        self.iterations  = iterations # Number of iterations for gradient descent algorithm

    def sigmoid(self,x):
        return 1/(1+np.exp(-x))



    def fit(self,train_x,train_y):
        sample_num = train_x.shape[0]
        sample_features = train_x.shape[1]

        # Setting initial weights and biases as 0
        self.weights = np.zeros(sample_features)
        self.bias = 0

        for i in range(self.iterations):
            result = np.dot(train_x,self.weights) + self.bias  # Calculating (WX + B)
            prediction = self.sigmoid(result)

            # Gradients
            dw = (1/sample_num) * np.dot(train_x.T , (prediction - train_y))
            db = (1/sample_num) * np.sum(prediction - train_y)

            # Weight and bias update
            self.weights = self.weights - self.lr * dw
            self.bias = self.bias - self.lr * db


    def predict(self,test_x):
        result = np.dot(test_x,self.weights) + self.bias
        prediction = self.sigmoid(result)
        for i in range(len(prediction)):
            if prediction[i] > 0.5:
                prediction[i] = 1
            else:
                prediction[i] = 0

        return prediction

class LogisticRegressionOVA: # Logistic Regression with 10 classes (One v All)
    def __init__(self,learning_Rate,iterations,class_num = 10):
        self.lr = learning_Rate
        self.iterations  = iterations
        self.models = 0
        self.class_num = class_num


    def fit(self,train_x,train_y):
        sample_num = train_x.shape[0]
        sample_features = train_x.shape[1]

        self.models = []

        # One model is trained for each class.
        # For each instance, if that instance belongs to class that we train, it gets 1 in train_Y list, otherwise 0.
        # train_Y is desired_class variable in this function.

        for i in range(self.class_num):

            desired_class = []
            for j in train_y:
                if j == i:
                    desired_class.append(1)
                else:
                    desired_class.append(0)
            desired_class = np.array(desired_class)

            model = LogisticRegression(self.lr,self.iterations)
            model.fit(train_x,desired_class)
            self.models.append(model)




    def predict(self,test_x):
        prediction = np.zeros((test_x.shape[0],len(self.models)))

        for i,model in enumerate(self.models):
            prediction[:,i] = model.predict(test_x)
        return np.argmax(prediction,axis=1)

"""3.1) Binary Classification"""

train_X_binary = np.array(train_img[:60])/255
train_X_binary = train_X_binary.reshape(60,120000)

train_Y_binary = []
for i in range(2):
    train_Y_binary.append(np.ones(30) * i)

train_Y_binary = np.array(train_Y_binary).flatten()

test_X_binary = np.array(query_img[:4])/255
test_X_binary = np.array(test_X_binary).reshape(4,120000)
test_Y_binary = []
for i in range(2):
    test_Y_binary.append(np.ones(2) * i)
test_Y_binary = np.array(test_Y_binary).flatten()

Lr_Binary = LogisticRegression(learning_Rate=0.001,iterations=1000)
Lr_Binary.fit(train_X_binary,train_Y_binary)

prediction = Lr_Binary.predict(test_X_binary)
print("Truth\tPrediction")
for i in range(len(prediction)):
    print(str(int(test_Y_binary[i]))+"\t"+str(int(prediction[i])))

m =confusion_matrix(test_Y_binary,prediction)
m

"""3.2) 10-Class Classification"""

train_X = np.array(train_img)/255
train_X = train_X.reshape(300,120000)

train_Y = []
for i in range(10):
    train_Y.append(np.ones(30) * i)

train_Y = np.array(train_Y).flatten()

Lr = LogisticRegressionOVA(learning_Rate= 0.001,iterations=1000)
Lr.fit(train_X,train_Y)

test_X = np.array(query_img)/255
test_X = test_X.reshape(20,120000)

test_Y = []
for i in range(10):
    test_Y.append(np.ones(2) * i)
test_Y = np.array(test_Y).flatten()


prediction = Lr.predict(test_X)

print("Truth\tPrediction")
for i in range(len(prediction)):
    if(test_Y[i] == prediction[i]):
        print(str(int(test_Y[i]))+"\t"+str(prediction[i]) + "\tMatch")
    else:

        print(str(int(test_Y[i]))+"\t"+str(prediction[i]))

confusion_matrix(test_Y,prediction)

